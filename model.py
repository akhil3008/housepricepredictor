# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rR5jhDILIbNYGuCYfzXhpKW27-_P4eWZ
"""

from pyspark.sql import SparkSession, SQLContext
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, Bucketizer
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline
import pyspark.sql.functions as F
from pyspark.ml.stat import Correlation
from pyspark.ml.regression import RandomForestRegressor, RandomForestRegressionModel
from pyspark.ml import PipelineModel
import os
import pandas as pd
import numpy as np
import sys


# import pip
# import pyspark
# pip install pyspark
# from pyspark import SparkConf, SparkContext
# from pyspark.sql.types import *
# from pyspark.ml.regression import LinearRegression
# from pyspark.mllib.evaluation import RegressionMetrics
# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
# from pyspark.sql.functions import *
# from pyspark.sql.functions import mean,stddev,min,col
# from pyspark.ml.linalg import Vectors
# from pyspark.sql.functions import udf, col,min


# import seaborn as sns
# import matplotlib as plt

#spark_session = SparkSession.builder.master("local[6]").appName("HousingRegressions").getOrCreate()
spark_session = SparkSession.builder.appName("HousingRegression").getOrCreate()
spark_context = spark_session.sparkContext
spark_sql_context = SQLContext(spark_context)

def training_model():
    env = "PROD"
    data = load_data()
    data = clean_data(data)
    data = preprocess(data)
    data = index_data(data)
    data = bucketize(data)
    assembledDF = data_corelation(data)
    # featureIndexer = index_features(assembledDF)
    train_model(assembledDF, env)

def load_data():
  path = ''
  filename = 'train.csv'
  if os.path.join(path, filename)==filename:
    data = spark_session.read.option('header', 'true').csv(os.path.join(path, filename), inferSchema=True)
    return data
  else:
    print("File Does not exist")

def get_int_data(data):
  print("get_int_data started")
  l_int =[]
  for item in data.dtypes:
      if item[1]=='int':
          l_int.append(item[0])
  return l_int 

def get_str_data(data):
  print("get_str_data started")
  l_str =[]
  for item in data.dtypes:
      if item[1]=='string':
          l_str.append(item[0])
  return l_str

def clean_data(data):
  print("clean_data started")
  junk_list = ['Id']
  int_col = get_int_data(data)
  # print(int_col)
  # identifying integer column records having less meaningful data
  for i in data.columns:
      if i in int_col:
          ct_total = data.select(i).count()
          ct_zeros = data.filter((F.col(i)== 0)).count()
      per_zeros =(ct_zeros / ct_total)*100
      # print('total count / zeros count '
      #       +i+' '+str(ct_total)+' / '+str(ct_zeros)+' / '+str(per_zeros))
      if(per_zeros > 75):
        junk_list.append(i)
  print(junk_list)
  data = data.drop(*junk_list)
  return data


def preprocess(data):
    print("preprocess started")
    data_stats_dict = {}
    int_d = []

    for item in data.dtypes:
        if item[1] == 'int':
            max_limit = data.select((F.mean(F.col(item[0])) + 5 * F.stddev(F.col(item[0])))).collect()[0][0]
            min_limit = data.select((F.mean(F.col(item[0])) - 5 * F.stddev(F.col(item[0])))).collect()[0][0]
            data_stats_dict.update({item[0]+"_max":int(max_limit)})
            data_stats_dict.update({item[0]+"_min":int(min_limit)})

    for item in data.dtypes:
        if item[1] == 'int':
            int_d.append(item[0])

    for c in int_d:
        data = data.where( (data[c] < data_stats_dict[c+"_max"] ) )
        data = data.where( (data[c] > data_stats_dict[c+"_min"] ) )

    return data


# n = {}
# # to be included
# n.createOrReplaceTempView("data_table")
#
# # to be included
# for col  in n.columns:
#   query = "SELECT COUNT(*) as count FROM data_table WHERE " + col + "=='NA' or " + col + "=='na' HAVING count>0"
#   sqlDF = spark_session.sql(query)
#   if (sqlDF.count()) > 0:
#     print(col, sqlDF)

def index_data(data):
    print("index_data started")
    feat_list = get_str_data(data)
    indexers = [StringIndexer(inputCol = column, outputCol = column+"_index").fit(data) for column in feat_list]
    pipeline = Pipeline(stages = indexers)
    df_feat = pipeline.fit(data).transform(data)
    df_feat = df_feat.drop(*feat_list)
    return df_feat

group_dict = {}
def split_bucket_size(min,max):
    print("split_bucket_size started")
    binsize = 32
    splitList = [float(min)]
    split = []
    bin_gap = ( max - min ) / binsize
    for x in range(1,binsize - 1):
      val = ( x * bin_gap ) + min
      for v in range(int((( (x - 1) * bin_gap ) + min)),int(val)):
        group_dict.update({v:x})
      splitList.append(float(val))
    splitList.append(float(max))
    return splitList

def fill_with_max(df_new, include):
    print("fill_with_max started")
    stats = df_new.agg(*(
        F.max(c).alias(c) for c in df_new.columns if c in [include]
    ))
    return stats.first().asDict()[include]

def fill_with_min(df_new, include):
    print("fill_with_min started")
    stats = df_new.agg(*(
        F.min(c).alias(c) for c in df_new.columns if c in [include]
    ))
    return stats.first().asDict()[include]

def bucketize(df_feat):
    import time

    print("Bucketing started")
    print(time.ctime())
    df_feat.createOrReplaceTempView("df_feat_table")
    unique_bucket_list = []
    for col in df_feat.columns:
        query = "SELECT count(distinct(" + col + ")) as count FROM df_feat_table HAVING count>32"
        sqlDF = spark_session.sql(query)
        if (sqlDF.count()) == 1 and (col != 'SalePrice'):
            unique_bucket_list.append(col)
    cols = unique_bucket_list

    model = Pipeline(stages=[
    Bucketizer(
        splits=split_bucket_size(fill_with_min(df_feat, x),fill_with_max(df_feat,  x )),
        inputCol=x, outputCol=x + "_bucket") for x in cols
]).fit(df_feat)
    df_feat = model.transform(df_feat)
    df_feat = df_feat.drop(*unique_bucket_list)
    print(time.ctime())
    return df_feat


def correlated_check(assembledDF, data):
    print("Correlation started")
    col =[]
    uncorrelated = []
    # correlation will be in Dense Matrix
    correlation = Correlation.corr(assembledDF,"features","pearson").collect()[0][0]
    # To convert Dense Matrix into DataFrame
    rows = correlation.toArray()

    # import matplotlib.pyplot as plt
    # fig, ax = plt.subplots(figsize=(30,30))
    # sns.heatmap(rows, annot=True, linewidths=.5, ax=ax)
    # print(rows)
    df_heat = pd.DataFrame(rows.tolist(),['MSSubClass',  'OverallQual',  'OverallCond',  'BsmtFullBath',  'FullBath',  'HalfBath',  'BedroomAbvGr',  'KitchenAbvGr',  'TotRmsAbvGrd',  'Fireplaces',  'GarageCars',  'MoSold',  'YrSold',  'SalePrice',  'MSZoning_index',  'Street_index',  'Alley_index',  'LotShape_index',  'LandContour_index',  'Utilities_index',  'LotConfig_index',  'LandSlope_index',  'Neighborhood_index',  'Condition1_index',  'Condition2_index',  'BldgType_index',  'HouseStyle_index',  'RoofStyle_index',  'RoofMatl_index',  'Exterior1st_index',  'Exterior2nd_index',  'MasVnrType_index',  'ExterQual_index',  'ExterCond_index',  'Foundation_index',  'BsmtQual_index',  'BsmtCond_index',  'BsmtExposure_index',  'BsmtFinType1_index',  'BsmtFinType2_index',  'Heating_index',  'HeatingQC_index',  'CentralAir_index',  'Electrical_index',  'KitchenQual_index',  'Functional_index',  'FireplaceQu_index',  'GarageType_index',  'GarageFinish_index',  'GarageQual_index',  'GarageCond_index',  'PavedDrive_index',  'SaleType_index',  'SaleCondition_index',  'LotArea_bucket',  'YearBuilt_bucket',  'YearRemodAdd_bucket',  'BsmtFinSF1_bucket',  'BsmtUnfSF_bucket',  'TotalBsmtSF_bucket',  '1stFlrSF_bucket',  '2ndFlrSF_bucket',  'GrLivArea_bucket',  'GarageArea_bucket',  'WoodDeckSF_bucket',  'OpenPorchSF_bucket',  'LotFrontage_index_bucket',  'MasVnrArea_index_bucket',  'GarageYrBlt_index_bucket'])
    df_heat = df_heat.T

    for j in range(0, len(df_heat)):
        if(df_heat['SalePrice'][j]<0 or df_heat['SalePrice'][j]>0.8):
          col.append(j)
        columns_list = np.array(col)
        columns_list_unique = np.unique(columns_list)
    for u in columns_list_unique:
        if df_heat.columns[u] != 'SalePrice':
          uncorrelated.append(df_heat.columns[u])
        new_list = data.drop(*uncorrelated)
    return new_list

def data_corelation(a):
    print("data_corelation started")
    # Create the VectorAssembler object
    data = a
    assembler = VectorAssembler(inputCols= a.columns, outputCol= "features")
    assembledDF = assembler.transform(a)
    assembledDF = correlated_check(assembledDF, data)
    return assembledDF

def train_model(assembledDF, env):
    os.environ[
        'HADOOP_HOME'] = "C:/Users/akhil.singireddy/Downloads/spark-3.2.1-bin-hadoop3.2/spark-3.2.1-bin-hadoop3.2/spark-3.2.1-bin-hadoop3.2/"
    sys.path.append(
        "C:/Users/akhil.singireddy/Downloads/spark-3.2.1-bin-hadoop3.2/spark-3.2.1-bin-hadoop3.2/spark-3.2.1-bin-hadoop3.2/bin")

    print("Training model")
    assembler = VectorAssembler(inputCols= assembledDF.columns, outputCol= "features")
    assembledDF = assembler.transform(assembledDF)

    featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexed_feature", maxCategories=4, handleInvalid='skip').fit(assembledDF)

    (trainingData, testData) = assembledDF.randomSplit([0.8, 0.2], seed=40)
    # Train a RandomForest model.
    rf = RandomForestRegressor(featuresCol="indexed_feature",labelCol='SalePrice')
    # Chain indexer and forest in a Pipeline
    pipeline = Pipeline(stages=[featureIndexer, rf])
    # Train model.  This also runs the indexer.
    model = pipeline.fit(trainingData)
    trainingData.show(1)
    rfModel = model.stages[1]
    print(rfModel)  # summary only
    # Make predictions.
    if env == "STAGE":
        predictions = model.transform(testData)
        # Select example rows to display.
        # predictions.select("prediction", "SalePrice", "features").show(25)

        # Select (prediction, true label) and compute test error
        evaluator = RegressionEvaluator(
             predictionCol="prediction", metricName="rmse", labelCol='SalePrice')
        rmse = evaluator.evaluate(predictions)
        print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

        evaluatorr2 = RegressionEvaluator(
             predictionCol="prediction", metricName="r2", labelCol='SalePrice')
        r2 = evaluatorr2.evaluate(predictions)
        print("R2 Score %g" % r2)

        rfModel = model.stages[1]
        print(rfModel)  # summary only
    else:
        model.save("pricing.model")

def predict(user_data, type):
    loadModel = PipelineModel.load("pricing.model")
    if type == "csv":
        df_userdata = spark_session.read.option('header', 'true').csv(os.path.join('', user_data , '.csv'),
                                                                      inferSchema=True)
        assembler_test = VectorAssembler(inputCols=df_userdata.columns,
                                         outputCol="features")
        assembledDF_test = assembler_test.transform(df_userdata)
    else:
        assembler_pred = VectorAssembler(inputCols=user_data.columns,
                                         outputCol="features")
        assembledDF_pred = assembler_pred.transform(user_data)
        predictions = loadModel.transform(assembledDF_pred)
        predictions.createOrReplaceTempView("predicted_data")
        predictions = spark_session.sql("SELECT prediction FROM predicted_data limit 1")
        predictions.collect()
        rows = [list(row) for row in predictions.collect()]
        return rows